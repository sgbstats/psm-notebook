---
title: "C-for-benefit"
bibliography: references.bib
---

In C for benefit, the matching is based on predicted benefit, but what happens when there are substantially different baseline 'hetrogeneous' risks? This is based on @klaveren2018 which used a logistic model.

#### Lit review

It appears from @klaveren2018 that (assuming equal number in each group; in a case of unequal numbers extra people are removed at random), each arm is ranked on predicted treatment benefit. We take $rank(\beta_{het,i})$ in each arm and rank 1 in treatment is paired with rank 1 in control, two with two etc.

Where there is a large homogeneous/baseline effect (that is the outcome is caused in large part by factor(s) that do not interact with treatment), the means that we may end up matching patients with equal/very similar

@efthimiou2023 add that C4B can be used by matching on covariates or on the predicted benefit. In a further paper by @hoogland2024 and @maas2023 discuss matching on covariates (via mahalinobis distance) but in high dimensional data, this yields issues with high sentimentality and that MD won't account for the weights of each predictor in the model and will consider each one equally. @klaveren2018 briefly mentioned using MD as a sensitivity analysis but didn't go into further detail. @smit2025 used a version called AUC-benefit although the definition was a tad ropey and didn't use C4B as they didn't like the matching.

In all the papers so far on the topic they all consider either binary or survival and have not yet extended their work to linear/count. A review of studies that have applied the C4B is linked [here](c4bwild.qmd).

### Model decomposition

Let's consider a model

$$
Y \sim X_1 + X_2 + arm * X_3 + arm * X_4 
$$

This can be rewritten as

$$
y_i= f(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} + arm_i*(\beta_a + \beta_{3a} x_{3i} + \beta_{4a} x_{4i}), ...) 
$$

where $...$ is other arguments as required. This can subsequently reduce

$$
y_i=f(\beta_0+ \beta_a arm_i+\beta_{homo,i}+\beta_{het,i}arm_i,...)
$$

$$
\beta_{homo,i}=\beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i}
$$

$$
\beta_{het,i}= \beta_{3a} x_{3i} + \beta_{4a} x_{4i}
$$

So in the case where $\beta_a \gg \beta_{het,i}$ and $\beta_{homo,i} \gg \beta_{het,i}$ , this reduces to essentially a homogeneous model and C-for-benefit doesn't really apply. We will further consider only when $\beta_{het,i}$ is non-negligible. This does apply to larger and more complex models but we use this model wlog. $\beta_0$ can be folded into $\beta_{homo,i}$ and $\beta_{arm}$ can be folded into $\beta_{het,i}$ as they shift the values equally for all patients.

Previous literature has generally used logistic regression with the ternary outcome (-1,0,1) corresponding to harm, no difference, and benefit respectively. In this analysis we extend the paired outcome to calculated as $actual_{trt,j}-actual_{control,j}$ where j is the $j^{th}$ matched pair. \<0 corresponds to harm and \>0 corresponds to benefit.

### Matching procedure

#### Option 1: Distance between the het and the homo

After reducing the model down to our homo- and hetrogeneous effects, we can match the patients using some distance metric.

A benefit of this is that the data can be shifted (e.g., folding in the intercept) and that it does not affect the match. The downside of this is that you cannot weight the data.

When calculating C4B you rank the difference in the actual outcome and the rank of the mean hetro value.

#### Option 2: Excess exacerbations

-   Matching only on the rank of the heterogeneous estimate

-   We calculate the excess exacerbations based on $actual_i-\mathbb{E}(exac_i|arm_i=control)$

-   In the case of a pt in the control arm in real life then it would be the difference between predicted and actual

-   In the case of pt in the trt group then the excess captures the treatment effect

-   You then calculate the C4B based on the $excess\sim rank(hetro)$.

    -   In this case the hetro values are likely to be a lot closer than when using homo as part of the match.

-   This will only work for count or linear models.

#### A side note on Random Forests

If we were to use the stratified random forests (the proper name I have forgotten), then $\beta_{homo}$ is the value of the control tree and the $\beta_{het}$ is the difference in the trees.

### Implementation {#sec-implement}

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\data/study_data.RDa")

source("C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\R\\prediction\\decompose_model.R")
source("C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\R\\prediction\\c4b.R")

set.seed(121)
data1=study_data$EFFECT$ad
data1$train=as.logical(rbinom(nrow(data1),1, 0.8))

data=data1
```

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)

# Data loaded out of sight
train=data %>% filter(train) %>%  mutate(rate=exac_modsev_n/trt_dur)
test=data %>% filter(!train) %>%  mutate(rate=exac_modsev_n/trt_dur)
formula=rate ~ arm_ipd*eos_bl+age_imp+exac_bl+sex+arm_ipd*smoking_bl+fev1_bl

model=glm.nb(formula, train, contrasts=NULL)

summary(model)

```

**Concordance in train**

```{r, message=FALSE, warning=FALSE}
#finding the rate adjusted concordance
train %>% 
  cbind.data.frame(pred=predict(model, newdata = train, type="response")) %>%
  mutate(rate=exac_modsev_n/trt_dur,
         rate_p=pred/trt_dur) %>% 
  lm(rate~rate_p, data=.) %>% 
  survival::concordance()


```

**Concordance in test**

```{r, message=FALSE, warning=FALSE}
test %>% 
  cbind.data.frame(pred=predict(model, newdata = test, type="response")) %>%
  mutate(rate=exac_modsev_n/trt_dur,
         rate_p=pred/trt_dur) %>% 
  lm(rate~rate_p, data=.) %>% 
  survival::concordance()
  
```

FWIW this is a weird trial that has almost no ICS effect.

#### Model decomposition

```{r, file="C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\R\\prediction\\decompose_model.R", echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
```

```{r}
# Usage
result <- decompose_model(model, "arm_ipd", leave_var_as_binary = T)
head(result$decomposed_model)

```

```{r, file="C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\R\\prediction\\c4b.R", echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
```

```{r,echo=TRUE, message=FALSE, warning=FALSE}
c4b(result, method="optimal")
c4b(result, method="excess")
```

##### Validation

```{r,echo=TRUE, message=FALSE, warning=FALSE}

result2 <- decompose_model(model, "arm_ipd", newdata=test, leave_var_as_binary = T)
head(result2$decomposed_model)
c4b(result2, method="optimal")
c4b(result2, method="excess")
```

### Non-paramteric appraoch

```{r, file="C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\R\\prediction\\decompose_model_non_parametric.R", echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
```

Note: that if you are using a custom model, e.g., submodels then it needs a structured return and a proper predict. Where you get a raw prediction (e.g., in a Random Forest) then you need to add a link function to get it in terms of a GLM.

One issue that arises when using non-parametric models is that where the glm would use some link function the non-parametric model may not which could lead to scaling issues. I think that the solution is to use a link function on the predict if linear predictors aren't returnable from the native predict.
