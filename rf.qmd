---
title: "Random Forests"
bibliography: references.bib
---

Random forest needs to have complete data and ones where there it is missing it seems to want to impute with mean impution. I really don't like this approach. @tang2017

Takes sqrt(n) of the variables each time and builds a decision tree from those variables and then takes the average from all of the trees (normally 100)

I am using the [regression imputation](ri) to build out the incomplete data.

Major question about interaction. I think that we would have add an interaction variable into the datasets but then theoretically it should only be in models when both of the arm and eos (eg) were both in the tree.

We are using a T-learner

```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("C:\\Users\\mbbx4sb5\\Dropbox (The University of Manchester)\\00_ICS_RECODE_Shared\\data/study_data.RDa")
```

```{r,message=FALSE, warning=FALSE, eval=F}


library(randomForest)
library(tidyverse)



ddm=study_data$EFFECT$ddm %>% 
  dplyr::select(usubjid, paramcd, aval) %>% 
  pivot_wider(names_from=paramcd, values_from=aval) %>% 
  dplyr::select(-BACT)

ad=study_data$EFFECT$ad %>% 
  dplyr::select(usubjid, arm_ipd, ics_dose, ics_dose_24, trt_dur, age_imp, ics_bl, sex, cat_bl,
                fev1_bl, fev1percent_bl, fvc_bl, fev1fvc_bl, laba_bl, lama_bl, comorb_tot_bl, eos_bl, 
                eos_pc_bl, reversibility_bl, smoking_bl, exac_bl, gold_grp, gold_stage, exac_modsev_n)
model="rate~."
model=as.formula(model)
data=merge(ad, ddm, by="usubjid") %>%
  filter(trt_dur>0.5) %>% 
  #dplyr::mutate_at(c("exac_bl", "fev1_bl"),~ifelse(sample(c(TRUE, FALSE), size = length(.), replace = TRUE, prob = c(0.8, 0.2)),., NA))%>% 
  mutate(interaction=eos_bl*if_else(arm_ipd=="ICS",1,0),
         rate=exac_modsev_n/trt_dur) %>% 
  drop_na(rate)%>% 
  dplyr::select(-trt_dur, -exac_modsev_n, -usubjid) %>% 
  mutate(across(where(is.logical), as.integer))

data2=na.roughfix(data)


data$train=as.logical(rbinom(nrow(data),1, 0.8))

rf=randomForest(formula=model, data=data %>% filter(train) %>% dplyr::select(-train), na.action = na.omit)
varImpPlot(rf)


newdata=data %>% filter(!train) %>% 
  dplyr::select(-train)%>%
  na.roughfix()

z=predict(rf, newdata, se.fit=T)

pred.out=data.frame(actual=newdata$rate,
                    pred=z)

pred.out %>% lm(actual~pred, data=.) %>% survival::concordance()



z1=predict(rf, newdata, predict.all = T)



pred=z1$individual%>% as.data.frame() %>%  mutate(rn=row_number()) %>% 
  pivot_longer(cols=-"rn", values_to="pred", names_to = "tree") %>% 
  summarise(se.fit=sd(pred,na.rm=T), .by="rn") %>% 
  merge(data.frame(pred=z1$aggregate )%>% mutate(rn=row_number()), by="rn")




```

## Thoughts about imputing data

-   na.roughfix, isn't a great option but may have some useful properties like actually wanting to reduce variance of predictor as a penalty for having high levels of incompleteness

-   How we would deal with missing data in the test set may be more tricky.

    -   I think we could use MICE to make n=20ish copies of the data to be predicted and then averaged over the prediction. Could be computationally intensive though (as then you be making 10k predictions.

    -   In practice you could end up having to impute on the client-side which may not be possible

    -   Or you use mean/mode imputation for new predictions.

::: {#refs}
:::